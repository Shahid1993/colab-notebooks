{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_completion_prediction_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahid1993/colab-notebooks/blob/master/word_completion_prediction_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU5MHFuEiKSJ",
        "colab_type": "text"
      },
      "source": [
        "# Testing Already Created Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKXljvAujD7y",
        "colab_type": "text"
      },
      "source": [
        "### Load Model from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bycjqNRjiIid",
        "colab_type": "code",
        "outputId": "1471ec7c-4dc3-4930-88f8-b12520b65ab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "# Mounting Google Drive to Load Data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe5fd94Kuth2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "import pickle\n",
        "import heapq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DDUxE02jHaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('./drive/My Drive/ML/Models/word_completion_prediction/word_completion_prediction_keras_model.h5')\n",
        "history = pickle.load(open(\"./drive/My Drive/ML/Models/word_completion_prediction/word_completion_prediction_history.p\", \"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uEfPhAUv6vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = ' !\"\\'(),-.0123456789:;?_abcdefghijklmnopqrstuvwxyz¦'\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc2bqryXZzyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_input(text):\n",
        "    x = np.zeros((1, len(text), len(chars)))\n",
        "    for t, char in enumerate(text):\n",
        "        x[0, t, char_indices[char]] = 1.\n",
        "        \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFSJJODFihiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, top_n=3):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds)\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    \n",
        "    return heapq.nlargest(top_n, range(len(preds)), preds.take)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uQTCh9nioeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_completion(text):\n",
        "    original_text = text\n",
        "    generated = text\n",
        "    completion = ''\n",
        "    while True:\n",
        "        x = prepare_input(text)\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        next_index = sample(preds, top_n=1)[0]\n",
        "        next_char = indices_char[next_index]\n",
        "        text = text[1:] + next_char\n",
        "        completion += next_char\n",
        "        \n",
        "        if len(original_text + completion) + 2 > len(original_text) and next_char == ' ':\n",
        "            return completion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC4jLUdsitKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_completions(text, n=3):\n",
        "    x = prepare_input(text)\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    next_indices = sample(preds, n)\n",
        "    return [indices_char[idx] + predict_completion(text[1:] + indices_char[idx]) for idx in next_indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzxwwuRNixKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# actual_text = [\n",
        "#     \"It is not a lack of love, but a lack of friendship that makes unhappy marriages.\",\n",
        "#     \"That which does not kill us makes us stronger.\",\n",
        "#     \"I'm not upset that you lied to me, I'm upset that from now on I can't believe you.\",\n",
        "#     \"And those who were seen dancing were thought to be insane by those who could not hear the music.\",\n",
        "#     \"It is hard enough to remember my opinions, without also remembering my reasons for them!\",\n",
        "#     \"A man lying on a comfortable sofa is listening to his wi\",\n",
        "#     \"Assuming the predictions are probabilistic, novel sequences can be generated from a trai\",\n",
        "#     \"The networks performance is competitive with state-of-the-art language models, and it works almost\",\n",
        "#     \"This document is the initial part of a study to predict next words from a text dataset\"\n",
        "# ]\n",
        "\n",
        "input = [\n",
        "    \"It is not a lack of lov\",\n",
        "    \"That which does not kill us makes us stro\",\n",
        "    \"I'm not upset that you lied to me, I'm upset that from now on I can't bel\",\n",
        "    \"And those who were seen dan\",\n",
        "    \"It is hard enough to remember my opini\",\n",
        "    \"A man lying on a comfortable ch\",\n",
        "    \"The networks perf\",\n",
        "    \"The networks performance is competi\",\n",
        "    \"The networks performance is competitive with state-of-the-art lan\",\n",
        "    \"This document is the initial part of a study to pre\",\n",
        "    \"This document is the initial part of a study to pred\",\n",
        "    \"Assuming the prediction\",\n",
        "    \"Assuming the predictions are probabilistic, novel sequences can be gene\",\n",
        "    \"Assuming the predictions are probabilistic, novel sequences can be generat\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sNhloMoi3nl",
        "colab_type": "code",
        "outputId": "7c8b230b-4561-48ac-abe2-56b6b9b2bc8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        }
      },
      "source": [
        "for i in input:\n",
        "    seq = i.lower()\n",
        "    print(seq)\n",
        "    print(predict_completions(seq, 5))\n",
        "    print()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it is not a lack of lov\n",
            "['e ', 'ical ', 'ality ', 'oure ', 'uling ']\n",
            "\n",
            "that which does not kill us makes us stro\n",
            "['ng ', 'dger ', 've ', 'gget ', 'w ']\n",
            "\n",
            "i'm not upset that you lied to me, i'm upset that from now on i can't bel\n",
            "['ieve ', 'ong ', 'aes ', 'ess ', 'low ']\n",
            "\n",
            "and those who were seen dan\n",
            "['gerous ', 'king ', 'ders ', 'y ', 'ce ']\n",
            "\n",
            "it is hard enough to remember my opini\n",
            "['on ', 'an ', 'ty ', 'fic ', 's ']\n",
            "\n",
            "a man lying on a comfortable ch\n",
            "['ild ', 'aracteristic ', 'ristian ', 'erristic ', 'omes ']\n",
            "\n",
            "the networks perf\n",
            "['ectly ', 'ord ', 'aind, ', 'iced ', 'uch ']\n",
            "\n",
            "the networks performance is competi\n",
            "['tion ', 'ce, ', 'ences ', 'sion ', 'ons ']\n",
            "\n",
            "the networks performance is competitive with state-of-the-art lan\n",
            "['ger ', 'ds ', 'k ', 'ce ', 'ture ']\n",
            "\n",
            "this document is the initial part of a study to pre\n",
            "['sent ', 'dicate ', 'cisely ', 'vail ', 'juce ']\n",
            "\n",
            "this document is the initial part of a study to pred\n",
            "['icate ', 'ention ', 'ucate ', 'action ', 'ocation ']\n",
            "\n",
            "assuming the prediction\n",
            "[' of ', ', ', 's ', '. ', 'al ']\n",
            "\n",
            "assuming the predictions are probabilistic, novel sequences can be gene\n",
            "['rally ', 'ution ', 'man ', 'st ', 'dations, ']\n",
            "\n",
            "assuming the predictions are probabilistic, novel sequences can be generat\n",
            "['ions, ', 'ed ', 'on, ', 'y ', 'ure ']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8lBEf5fzVsf",
        "colab_type": "text"
      },
      "source": [
        "# Corpus Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKk-brP3i9i0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b4f97152-9a73-4756-90f3-2d9ac8bd848d"
      },
      "source": [
        "#path = 'nietzsche.txt'\n",
        "\n",
        "#path = \"./drive/My Drive/ML/data/nietzsche.txt\"\n",
        "\n",
        "#path = \"./drive/My Drive/ML/data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00001-of-00100\"\n",
        "\n",
        "path = \"./drive/My Drive/ML/data/word_pred.txt\"\n",
        "\n",
        "text = open(path).read().lower()\n",
        "print('corpus length:', len(text))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus length: 11646654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVHqSJwCzav0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "e4044260-7347-486b-fffa-338a70cb94bf"
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "print(f'unique chars: {len(chars)}')\n",
        "\n",
        "print(chars)\n",
        "\n",
        "print(''.join(map(str, chars)))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique chars: 71\n",
            "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '?', '@', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¤', '¦', '©', '«', 'ã', 'ä', '’', '“', '”', '†']\n",
            "\n",
            " !\"$%&'()*+,-.0123456789:;<=?@[]_`abcdefghijklmnopqrstuvwxyz¤¦©«ãä’“”†\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpvKmmrWzfhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(data):\n",
        "    punct = '\\n#$<=>[\\\\]@^{|}~¡¢£¤¥©«¬®°²´µ¶·º»¼½¾¿×àáâãäåæçèéêëíîïñóôõöøùúüþąćĕěœšŵžʼ˚а‎‐‑‚‟†•′₤€∆④●♥ﬁ（）￡�'\n",
        "    \n",
        "    for p in punct:\n",
        "        data = data.replace(p, '')\n",
        "        \n",
        "    return data\n",
        "  \n",
        "text = preprocess(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOTjX5nYzkjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "1d456f5c-a023-4799-d82e-578f50ba9b05"
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "print(f'unique chars: {len(chars)}')\n",
        "\n",
        "print(chars)\n",
        "\n",
        "print(''.join(map(str, chars)))\n",
        "\n",
        "print('corpus length:', len(text))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique chars: 58\n",
            "[' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¦', '’', '“', '”']\n",
            " !\"%&'()*+,-.0123456789:;?_`abcdefghijklmnopqrstuvwxyz¦’“”\n",
            "corpus length: 11464282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0wt6ikjznri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        " \n",
        "trainer = PunktTrainer()\n",
        "trainer.INCLUDE_ALL_COLLOCS = True\n",
        "trainer.train(text)\n",
        " \n",
        "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxuLq4110r5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f50b8669-3c5a-403b-f788-84428929315e"
      },
      "source": [
        "# Test the tokenizer on a piece of text\n",
        "sentences = \"Mr. James told me Dr. Brown is not available today. I will try tomorrow.\"\n",
        " \n",
        "print (tokenizer.tokenize(sentences))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Mr.', 'James told me Dr.', 'Brown is not available today.', 'I will try tomorrow.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJDi8QQc2GDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDMFjiKo2WHH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "da0928df-6560-4d90-9395-3a3ceebcd481"
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> Download\n",
            "Command 'Download' unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: punkt\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n",
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PyP6wsT2bxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import sent_tokenize\n",
        "sentences = sent_tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA82UtN022tW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "776d404b-81cc-40ff-daef-d0f14ff0bcc3"
      },
      "source": [
        "sentences[101]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"they have to rebuild .i also like everyday vanilla ice cream with the sides of the sandwich rolled in flaked coconut ( do this just after you fill the cookies so the ice cream is still soft enough for the flakes to adhere ) .the data showed a ratio of 2.9 birth defects per 1,000 live births in kettleman city during those years .still , the hornets , with hilton armstrong starting at center for chandler ( ankle ) , went toe-to-toe with the nuggets until denver \\'s third-quarter run started the celebration .rouen , france , feb .a definitive destination for advertisements from football \\'s biggest night.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY-Q0tYi28Fv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e4668e8e-5aba-4264-b8ee-1a02425a20a9"
      },
      "source": [
        "sentences[500]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'prudent and appropriate thing for chrysler to do to engage in the filings that they -- that received some notice a while back because they had to prepare for possible contingencies .mr fallon , 42 , two other riders and three other people were cleared after a key witness was undermined .under the normal rules of capitalism , any industry that can produce double-digit annual growth should soon be swamped by eager competitors until returns are driven down .the patient , villimin colleti , 71 , was taken to coney island hospital suffering from heart and brain damage , said the office of brooklyn district attorney charles j. hynes .some criminal always breaks the law and has a gun .the yankees star said the cousin told him it would give him a \" dramatic energy boost \" and repeatedly injected him from 2001-03 .liberal democrat mp evan harris says he has cross-party support for his measure to remove major discriminatory restrictions from the 1701 act of settlement , the independent reported friday .in an interview , melinda gates said , \" our money is tiny , \" despite the billions of dollars of resources available to her organization .but more remains to be done .the scale of some self-published sequels to the harry potter series is astonishing , though not their literary quality .mum and i lie together on the bed in her nursing home .the scheme was developed under the guidance of independent charity futurelab , which assesses the future of education and technology and develops resources to support new approaches to learning .he also said it \\'s important to return to fiscal sustainability , which would not include government stimulus efforts .national polling suggests obama retains a steady but statistically significant edge .mowaffak al-rubaie , the national security adviser , declined to comment when reached tuesday night .a federal judge on monday ordered dow chemical co. to pay  653 million and the former rockwell international corp.  508 million in compensatory damages , but capped the amount to be collected at  725 million .that was when i knew he was safe , \" he said .few really knew the man behind the reputation .uwe bought a 70 acre site next to its north bristol campus last october .then there was the hair-breadth margin of victory : just one tenth of one percentage point , the narrowest winning margin in the 20th century .but the best way to avoid ever having to use an obscure defense is to avoid having to go to court at all .karzai appears to be on a collision course with the international community over the timing of parliamentary elections .for a while , it looked like the world series champions might not make it .analysts said it was possible that , after talks and further research , the commission would decide against the need for any remedy with its final report next month .on the other hand , the benchmark 10-year note gained ground , with yields down about 2 basis points to 3.32 percent .local employment agencies including routes to work and jobcentre plus , and charities such as women \\'s aid , will refer clients to dress for success when they have secured a job interview .he said scouts have asked him about the injury , but only to inquire if he thinks it will affect him as a player , which he doesn \\'t .bryant purvis and an unidentified juvenile remain charged with attempted murder and conspiracy to commit murder .later in life i became pregnant .\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9nZo4lU3Gjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(42)\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import LSTM, Dropout, CuDNNLSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
        "from keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import sys\n",
        "import heapq\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybjaR9y35GK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "12784d97-2619-4517-f894-4c84cc13bb13"
      },
      "source": [
        "#path = 'nietzsche.txt'\n",
        "\n",
        "#path = \"./drive/My Drive/ML/data/nietzsche.txt\"\n",
        "\n",
        "#path = \"./drive/My Drive/ML/data/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00001-of-00100\"\n",
        "\n",
        "path = \"./drive/My Drive/ML/data/word_pred.txt\"\n",
        "\n",
        "text = open(path).read().lower()\n",
        "print('corpus length:', len(text))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus length: 11646654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upu1eWTd5IZf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "aed16e7a-1150-4c05-f7e9-6e52bcd332a6"
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "print(f'unique chars: {len(chars)}')\n",
        "\n",
        "print(chars)\n",
        "\n",
        "print(''.join(map(str, chars)))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique chars: 71\n",
            "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '?', '@', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¤', '¦', '©', '«', 'ã', 'ä', '’', '“', '”', '†']\n",
            "\n",
            " !\"$%&'()*+,-.0123456789:;<=?@[]_`abcdefghijklmnopqrstuvwxyz¤¦©«ãä’“”†\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV-2aZVA5MHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(data):\n",
        "    punct = '\\n#$<=>[\\\\]@^{|}~¡¢£¤¥©«¬®°²´µ¶·º»¼½¾¿×àáâãäåæçèéêëíîïñóôõöøùúüþąćĕěœšŵžʼ˚а‎‐‑‚‟†•′₤€∆④●♥ﬁ（）￡�'\n",
        "    \n",
        "    for p in punct:\n",
        "        data = data.replace(p, '')\n",
        "        \n",
        "    return data\n",
        "  \n",
        "text = preprocess(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxQZyqds5Pfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "9523ba72-40ed-415b-b862-0fc588d1e11c"
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "print(f'unique chars: {len(chars)}')\n",
        "\n",
        "print(chars)\n",
        "\n",
        "print(''.join(map(str, chars)))\n",
        "\n",
        "print('corpus length:', len(text))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique chars: 58\n",
            "[' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¦', '’', '“', '”']\n",
            " !\"%&'()*+,-.0123456789:;?_`abcdefghijklmnopqrstuvwxyz¦’“”\n",
            "corpus length: 11464282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgSgyCWw5TIn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "153d3c48-133a-4483-8998-46e66b0d98e9"
      },
      "source": [
        "SEQUENCE_LENGTH = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - SEQUENCE_LENGTH, step):\n",
        "    sentences.append(text[i: i + SEQUENCE_LENGTH])\n",
        "    next_chars.append(text[i + SEQUENCE_LENGTH])\n",
        "print(f'num training examples: {len(sentences)}')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num training examples: 3821414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diEAJLRU5XUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.zeros((len(sentences), SEQUENCE_LENGTH, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jm99P4N5Z_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "#model.add(LSTM(128, input_shape=(SEQUENCE_LENGTH, len(chars))))\n",
        "\n",
        "#model.add(CuDNNLSTM(128, input_shape=(None, len(chars))))\n",
        "\n",
        "model.add(CuDNNLSTM(128, input_shape=(None, len(chars)), return_sequences=True))\n",
        "#model.add(CuDNNLSTM(256, return_sequences=True))\n",
        "model.add(CuDNNLSTM(256))\n",
        "\n",
        "#Dropout added to avoid overfitting\n",
        "model.add(Dropout(rate = 0.2))\n",
        "\n",
        "# build model using keras documentation recommended optimizer initialization\n",
        "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSp2H6ad5b6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "a506884f-1fef-4bab-dd26-a64dca18953a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cu_dnnlstm_4 (CuDNNLSTM)     (None, None, 128)         96256     \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_5 (CuDNNLSTM)     (None, 256)               395264    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 58)                14906     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 58)                0         \n",
            "=================================================================\n",
            "Total params: 506,426\n",
            "Trainable params: 506,426\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcjuFP517TwJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "0c91fc62-c829-4874-b317-84a4e71950d9"
      },
      "source": [
        "#optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "history = model.fit(X, y, validation_split=0.05, batch_size=128, epochs=10, shuffle=True).history"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3630343 samples, validate on 191071 samples\n",
            "Epoch 1/10\n",
            "3630343/3630343 [==============================] - 812s 224us/step - loss: 1.2766 - acc: 0.6226 - val_loss: 1.0355 - val_acc: 0.6871\n",
            "Epoch 2/10\n",
            "3630343/3630343 [==============================] - 805s 222us/step - loss: 1.0719 - acc: 0.6802 - val_loss: 0.9915 - val_acc: 0.7020\n",
            "Epoch 3/10\n",
            "3630343/3630343 [==============================] - 801s 221us/step - loss: 1.0408 - acc: 0.6892 - val_loss: 0.9775 - val_acc: 0.7075\n",
            "Epoch 4/10\n",
            "3630343/3630343 [==============================] - 800s 220us/step - loss: 1.0271 - acc: 0.6939 - val_loss: 1.0191 - val_acc: 0.7080\n",
            "Epoch 5/10\n",
            "3630343/3630343 [==============================] - 811s 223us/step - loss: 1.0300 - acc: 0.6962 - val_loss: 0.9596 - val_acc: 0.7107\n",
            "Epoch 6/10\n",
            "3630343/3630343 [==============================] - 816s 225us/step - loss: 1.0167 - acc: 0.6980 - val_loss: 0.9534 - val_acc: 0.7135\n",
            "Epoch 7/10\n",
            "3630343/3630343 [==============================] - 814s 224us/step - loss: 1.0034 - acc: 0.7002 - val_loss: 0.9504 - val_acc: 0.7147\n",
            "Epoch 8/10\n",
            "3630343/3630343 [==============================] - 836s 230us/step - loss: 1.0016 - acc: 0.7015 - val_loss: 0.9556 - val_acc: 0.7154\n",
            "Epoch 9/10\n",
            "3630343/3630343 [==============================] - 834s 230us/step - loss: 1.1083 - acc: 0.6974 - val_loss: 0.9558 - val_acc: 0.7153\n",
            "Epoch 10/10\n",
            "3630343/3630343 [==============================] - 833s 230us/step - loss: 1.0581 - acc: 0.7004 - val_loss: 0.9924 - val_acc: 0.7139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxJ2FLmr7aLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('./drive/My Drive/ML/Models/word_completion_prediction/R3/word_completion_prediction_keras_model.h5')\n",
        "pickle.dump(history, open(\"./drive/My Drive/ML/Models/word_completion_prediction/R3/word_completion_prediction_history.p\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhJ7wPmvF_s_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('./drive/My Drive/ML/Models/word_completion_prediction/R3/word_completion_prediction_keras_model.h5')\n",
        "history = pickle.load(open(\"./drive/My Drive/ML/Models/word_completion_prediction/R3/word_completion_prediction_history.p\", \"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_HwdXtVGCCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_input(text):\n",
        "    x = np.zeros((1, len(text), len(chars)))\n",
        "    for t, char in enumerate(text):\n",
        "        x[0, t, char_indices[char]] = 1.\n",
        "        \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hZqtzwiGFCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, top_n=3):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds)\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    \n",
        "    return heapq.nlargest(top_n, range(len(preds)), preds.take)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtCHypN1GHt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_completions(text, n=3):\n",
        "    x = prepare_input(text)\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    next_indices = sample(preds, n)\n",
        "    return [indices_char[idx] + predict_completion(text[1:] + indices_char[idx]) for idx in next_indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EerG_sPgGK9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# actual_text = [\n",
        "#     \"It is not a lack of love, but a lack of friendship that makes unhappy marriages.\",\n",
        "#     \"That which does not kill us makes us stronger.\",\n",
        "#     \"I'm not upset that you lied to me, I'm upset that from now on I can't believe you.\",\n",
        "#     \"And those who were seen dancing were thought to be insane by those who could not hear the music.\",\n",
        "#     \"It is hard enough to remember my opinions, without also remembering my reasons for them!\",\n",
        "#     \"A man lying on a comfortable sofa is listening to his wi\",\n",
        "#     \"Assuming the predictions are probabilistic, novel sequences can be generated from a trai\",\n",
        "#     \"The networks performance is competitive with state-of-the-art language models, and it works almost\",\n",
        "#     \"This document is the initial part of a study to predict next words from a text dataset\"\n",
        "# ]\n",
        "\n",
        "input = [\n",
        "    \"It is not a lack of lov\",\n",
        "    \"That which does not kill us makes us stro\",\n",
        "    \"I'm not upset that you lied to me, I'm upset that from now on I can't bel\",\n",
        "    \"And those who were seen dan\",\n",
        "    \"It is hard enough to remember my opini\",\n",
        "    \"A man lying on a comfortable ch\",\n",
        "    \"Assuming the pre\",\n",
        "    \"The networks performance is competi\",\n",
        "    \"The networks performance is competitive with state-of-the-art lan\",\n",
        "    \"This document is the initial part of a study to pre\",\n",
        "    \"This document is the initial part of a study to pred\",\n",
        "    \"Assuming the prediction\",\n",
        "    \"Assuming the predictions are probabilistic, novel sequences can be gene\",\n",
        "    \"Assuming the predictions are probabilistic, novel sequences can be generat\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56pk9UEzGNa4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "outputId": "0807c16c-1dc0-46eb-c988-8549fbf9817c"
      },
      "source": [
        "for i in input:\n",
        "    seq = i.lower()\n",
        "    print(seq)\n",
        "    print(predict_completions(seq, 5))\n",
        "    print()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it is not a lack of lov\n",
            "['ed ', 'ing ', ' .a ', 's ', 'anis ']\n",
            "\n",
            "that which does not kill us makes us stro\n",
            "['nger ', 'ller ', 'den ', 'te ', 'om ']\n",
            "\n",
            "i'm not upset that you lied to me, i'm upset that from now on i can't bel\n",
            "['ong ', 'ieve ', 'l ', 'ess ', 'rear ']\n",
            "\n",
            "and those who were seen dan\n",
            "['cing ', 'gerous ', 'd ', ' in ', 'ment ']\n",
            "\n",
            "it is hard enough to remember my opini\n",
            "['on ', 'ng ', 'ty ', 'st ', 'al ']\n",
            "\n",
            "a man lying on a comfortable ch\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['air ', 'eckered ', 'ild ', 'ristmas ', 'urch ']\n",
            "\n",
            "assuming the pre\n",
            "['sent ', 'paration ', 'cise ', 'asure ', 'tent ']\n",
            "\n",
            "the networks performance is competi\n",
            "['ng ', 'tion ', 's ', 'cing ', 'on ']\n",
            "\n",
            "the networks performance is competitive with state-of-the-art lan\n",
            "['ds ', 'gers ', 'ese ', 'ter ', 'chers ']\n",
            "\n",
            "this document is the initial part of a study to pre\n",
            "['sent ', 'pare ', 'tend ', 'ase ', 'cise ']\n",
            "\n",
            "this document is the initial part of a study to pred\n",
            "['uce ', 'ical ', 'om ', 'ent ', 'se ']\n",
            "\n",
            "assuming the prediction\n",
            "[' of ', 's ', 'a ', 'ing ', 'ed ']\n",
            "\n",
            "assuming the predictions are probabilistic, novel sequences can be gene\n",
            "['rally ', 'd ', 'ment ', 'nded ', 'ther ']\n",
            "\n",
            "assuming the predictions are probabilistic, novel sequences can be generat\n",
            "['ion ', 'ure ', 'ed ', 'ory ', 'ly ']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GlzxCKUGQbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}